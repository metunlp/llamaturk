# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language

[Paper](https://arxiv.org/)

[Published Models: LlamaTurk](https://huggingface.co/metunlp)

[Instruction set used in instruction fine-tuning](llamaturk_instruction_set.json)

[Task-specific dataset (sentiment analysis) used in tasks-specific fine-tuning](https://huggingface.co/datasets/maydogan/TRSAv1)

[Benchmark dataset: Belebele](https://huggingface.co/datasets/facebook/belebele/viewer/default/tur_Latn)

[Benchmark dataset: Belebele](https://huggingface.co/datasets/xcopa/viewer/tr)

[Source code for instruction fine-tuning]()

[Source code for task-specific fine-tuning]()

[Source code for continual training]()

[Source code for vocabulary extension]()


Citation:
```
@InProceedings{toraman2022large,
  author    = {Toraman, Cagri},
  title     = {LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language},
  booktitle = {},
  month     = {},
  year      = {2024},
  address   = {},
  publisher = {},
  pages     = {},
  url       = {}
}
```
