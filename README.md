# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language

[Paper](https://arxiv.org/)

[Published Models (to run PEFT models, see inference code below)](https://huggingface.co/metunlp)

[Instruction set used in instruction fine-tuning](llamaturk_instruction_set.json)

[Dataset used in tasks-specific fine-tuning](https://huggingface.co/datasets/maydogan/TRSAv1)

[Evaluation benchmark dataset: Belebele](https://huggingface.co/datasets/facebook/belebele/viewer/default/tur_Latn)

[Evaluation benchmark dataset: XCOPA](https://huggingface.co/datasets/xcopa/viewer/tr)

[Source code for instruction fine-tuning](src/finetune_instruction.py)

[Source code for task-specific fine-tuning](src/finetune_task.py)

[Source code for continual training](src/continual_train.py)

[Source code for vocabulary extension](src/vocabulary_extension.py)

[Source code for model inference](src/inference.py)

Citation:
```
@InProceedings{toraman2022large,
  author    = {Toraman, Cagri},
  title     = {LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language},
  booktitle = {},
  month     = {},
  year      = {2024},
  address   = {},
  publisher = {},
  pages     = {},
  url       = {}
}
```
